{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c273b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# Load and split PDF\n",
    "loader = PyPDFLoader('rag_vs_fine_tuning.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
    "docs = splitter.split_documents(data)\n",
    "\n",
    "# Embed and persist\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma.from_documents(docs, embedding=embedding_function, persist_directory=os.getcwd())\n",
    "\n",
    "# ✅ Define retriever\n",
    "#retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # <--- not \"similarity\"\n",
    "    search_kwargs={\"k\": 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b44926b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Retrieved Chunk 0 ---\n",
      "\n",
      "RAG vs. ﬁne-tuning \n",
      "Retrieval augmented generation (RAG) and ﬁne-tuning are two methods enterprises can \n",
      "use to get more value out of large language models (LLMs). Both work by tailoring the LLM \n",
      "to the speciﬁc use cases, but the methodologies behind them diƯer signiﬁcantly.\n",
      "\n",
      "--- Retrieved Chunk 1 ---\n",
      "\n",
      "RAG vs. ﬁne-tuning \n",
      "Retrieval augmented generation (RAG) and ﬁne-tuning are two methods enterprises can \n",
      "use to get more value out of large language models (LLMs). Both work by tailoring the LLM \n",
      "to the speciﬁc use cases, but the methodologies behind them diƯer signiﬁcantly.\n",
      "\n",
      "--- Retrieved Chunk 2 ---\n",
      "\n",
      "RAG vs. ﬁne-tuning \n",
      "Retrieval augmented generation (RAG) and ﬁne-tuning are two methods enterprises can \n",
      "use to get more value out of large language models (LLMs). Both work by tailoring the LLM \n",
      "to the speciﬁc use cases, but the methodologies behind them diƯer signiﬁcantly.\n",
      "\n",
      "--- Retrieved Chunk 3 ---\n",
      "\n",
      "Watch the latest podcast episodes  \n",
      "What is retrieval augmented generation (RAG)? \n",
      "RAG is an LLM optimization method introduced by Meta AI in a 2020 paper called \n",
      "\"Retrieval-Augmented Generation for Knowledge-Intensive Tasks\" .[1] It is a data architecture\n",
      "\n",
      "--- Retrieved Chunk 4 ---\n",
      "\n",
      "Watch the latest podcast episodes  \n",
      "What is retrieval augmented generation (RAG)? \n",
      "RAG is an LLM optimization method introduced by Meta AI in a 2020 paper called \n",
      "\"Retrieval-Augmented Generation for Knowledge-Intensive Tasks\" .[1] It is a data architecture\n",
      "\n",
      "--- Retrieved Chunk 5 ---\n",
      "\n",
      "RAG vs. ﬁne-tuning \n",
      "Retrieval augmented generation (RAG) and ﬁne-tuning are two methods enterprises can \n",
      "use to get more value out of large language models (LLMs). Both work by tailoring the LLM \n",
      "to the speciﬁc use cases, but the methodologies behind them diƯer signiﬁcantly. \n",
      "Though generative AI has come a long way since its inception, the task of generating \n",
      "automated responses in real time to user queries is still a signiﬁcant challenge. As \n",
      "enterprises race to incorporate gen AI into their processes to reduce costs, streamline\n",
      "\n",
      "--- Retrieved Chunk 6 ---\n",
      "\n",
      "What is ﬁne-tuning? \n",
      "Fine-tuning is the process of retraining a pretrained model on a smaller, more focused set \n",
      "of training data to give it domain-speciﬁc knowledge. The model then adjusts its \n",
      "parameters—the guidelines governing its behavior—and its embeddings to better ﬁt the \n",
      "speciﬁc data set.\n",
      "\n",
      "--- Retrieved Chunk 7 ---\n",
      "\n",
      "How does RAG work? \n",
      "Retrieval augmented generation works by locating information in internal data sources that \n",
      "is relevant to the user’s query, then using that data to generate more accurate responses. A \n",
      "data \"retrieval\" mechanism is added to \"augment\" the LLM by helping it \"generate\" more\n",
      "\n",
      "--- Retrieved Chunk 8 ---\n",
      "\n",
      "RAG vs. ﬁne-tuning \n",
      "Retrieval augmented generation (RAG) and ﬁne-tuning are two methods enterprises can \n",
      "use to get more value out of large language models (LLMs). Both work by tailoring the LLM \n",
      "to the speciﬁc use cases, but the methodologies behind them diƯer signiﬁcantly. \n",
      "Though generative AI has come a long way since its inception, the task of generating \n",
      "automated responses in real time to user queries is still a signiﬁcant challenge. As \n",
      "enterprises race to incorporate gen AI into their processes to reduce costs, streamline \n",
      "workﬂows and stay ahead of competitors, they often struggle with getting their chatbots \n",
      "and other models to reliably generate accurate answers. \n",
      "What’s the diƯerence between RAG and ﬁne-tuning? \n",
      "The diƯerence between RAG and ﬁne-tuning is that RAG augments a natural language \n",
      "processing (NLP) model by connecting it to an organization’s proprietary database, while \n",
      "ﬁne-tuning optimizes deep learning models for domain-speciﬁc tasks. RAG and ﬁne-tuning \n",
      "have the same intended outcome: enhancing a model’s performance to maximize value for \n",
      "the enterprise that uses it. \n",
      "RAG uses an organization’s internal data to augment prompt engineering, while ﬁne-tuning \n",
      "retrains a model on a focused set of external data to improve performance. \n",
      " \n",
      "The latest AI News + Insights   \n",
      "Discover expertly curated insights and news on AI, cloud and more in the weekly Think \n",
      "Newsletter.  \n",
      "Subscribe today \n",
      "Why are RAG and ﬁne-tuning important?\n",
      "\n",
      "--- Retrieved Chunk 9 ---\n",
      "\n",
      "Retrieval augmented generation works by locating information in internal data sources that \n",
      "is relevant to the user’s query, then using that data to generate more accurate responses. A \n",
      "data \"retrieval\" mechanism is added to \"augment\" the LLM by helping it \"generate\" more \n",
      "relevant responses. \n",
      "RAG models generate answers via a four-stage process: \n",
      "1. Query: A user submits a query, which initializes the RAG system. \n",
      "2. Information retrieval: Complex algorithms comb the organization’s knowledge \n",
      "bases in search of relevant information. \n",
      "3. Integration: The retrieved data is combined with the user’s query and given to the \n",
      "RAG model to answer. Up to this point, the LLM has not processed the query.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the difference between retrieval-augmented generation and fine-tuning?\"\n",
    "# Use invoke instead of deprecated method\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "# Print contents\n",
    "#for i, doc in enumerate(relevant_docs):\n",
    "#    print(f\"\\n--- Document {i+1} ---\\n\")\n",
    "#    print(doc.page_content)\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    print(f\"\\n--- Retrieved Chunk {i} ---\\n\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e68f747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Retrieved Chunk 0 ---\n",
      "\n",
      "relevant responses. \n",
      "RAG models generate answers via a four-stage process: \n",
      "1. Query: A user submits a query, which initializes the RAG system. \n",
      "2. Information retrieval: Complex algorithms comb the organization’s knowledge \n",
      "bases in search of relevant information. \n",
      "3. Integration: The retrieved data is combined with the user’s query and given to the \n",
      "RAG model to answer. Up to this point, the LLM has not processed the query.\n",
      "\n",
      "--- Retrieved Chunk 1 ---\n",
      "\n",
      "relevant responses. \n",
      "RAG models generate answers via a four-stage process: \n",
      "1. Query: A user submits a query, which initializes the RAG system. \n",
      "2. Information retrieval: Complex algorithms comb the organization’s knowledge \n",
      "bases in search of relevant information.\n",
      "\n",
      "--- Retrieved Chunk 2 ---\n",
      "\n",
      "relevant responses. \n",
      "RAG models generate answers via a four-stage process: \n",
      "1. Query: A user submits a query, which initializes the RAG system. \n",
      "2. Information retrieval: Complex algorithms comb the organization’s knowledge \n",
      "bases in search of relevant information.\n",
      "\n",
      "--- Retrieved Chunk 3 ---\n",
      "\n",
      "relevant responses. \n",
      "RAG models generate answers via a four-stage process: \n",
      "1. Query: A user submits a query, which initializes the RAG system. \n",
      "2. Information retrieval: Complex algorithms comb the organization’s knowledge \n",
      "bases in search of relevant information.\n",
      "\n",
      "--- Retrieved Chunk 4 ---\n",
      "\n",
      "Retrieval augmented generation works by locating information in internal data sources that \n",
      "is relevant to the user’s query, then using that data to generate more accurate responses. A \n",
      "data \"retrieval\" mechanism is added to \"augment\" the LLM by helping it \"generate\" more \n",
      "relevant responses. \n",
      "RAG models generate answers via a four-stage process: \n",
      "1. Query: A user submits a query, which initializes the RAG system. \n",
      "2. Information retrieval: Complex algorithms comb the organization’s knowledge \n",
      "bases in search of relevant information. \n",
      "3. Integration: The retrieved data is combined with the user’s query and given to the \n",
      "RAG model to answer. Up to this point, the LLM has not processed the query.\n",
      "\n",
      "--- Retrieved Chunk 5 ---\n",
      "\n",
      "to it. RAG models can return more accurate answers with the added context of internal \n",
      "data than they otherwise would be able to without it. \n",
      "A ﬁne-tuned model typically outperforms its corresponding base model, such as GPT-3 or\n",
      "\n",
      "--- Retrieved Chunk 6 ---\n",
      "\n",
      "\"Retrieval-Augmented Generation for Knowledge-Intensive Tasks\" .[1] It is a data architecture \n",
      "framework that connects an LLM to an organization’s proprietary data, often stored in data \n",
      "lakehouses. These vast data platforms are dynamic and contain all the data moving \n",
      "through the organization across all touchpoints, both internal and external. \n",
      "How does RAG work? \n",
      "Retrieval augmented generation works by locating information in internal data sources that \n",
      "is relevant to the user’s query, then using that data to generate more accurate responses. A \n",
      "data \"retrieval\" mechanism is added to \"augment\" the LLM by helping it \"generate\" more \n",
      "relevant responses. \n",
      "RAG models generate answers via a four-stage process: \n",
      "1. Query: A user submits a query, which initializes the RAG system. \n",
      "2. Information retrieval: Complex algorithms comb the organization’s knowledge \n",
      "bases in search of relevant information. \n",
      "3. Integration: The retrieved data is combined with the user’s query and given to the\n",
      "\n",
      "--- Retrieved Chunk 7 ---\n",
      "\n",
      "How does RAG work? \n",
      "Retrieval augmented generation works by locating information in internal data sources that \n",
      "is relevant to the user’s query, then using that data to generate more accurate responses. A \n",
      "data \"retrieval\" mechanism is added to \"augment\" the LLM by helping it \"generate\" more\n",
      "\n",
      "--- Retrieved Chunk 8 ---\n",
      "\n",
      "engineers must build the data pipelines needed to connect their organization’s data \n",
      "lakehouses with the LLM. \n",
      "To conceptualize RAG, imagine a gen AI model as an amateur home cook. They know the \n",
      "basics of cooking, but lack the expert knowledge—an organization’s proprietary \n",
      "database—of a chef trained in a particular cuisine. RAG is like giving the home cook a \n",
      "cookbook for that cuisine. By combining their general knowledge of cooking with the \n",
      "recipes in the cookbook, the home cook can create their favorite cuisine-speciﬁc dishes \n",
      "with ease. \n",
      "The RAG data retrieval process\n",
      "\n",
      "--- Retrieved Chunk 9 ---\n",
      "\n",
      "bases in search of relevant information. \n",
      "3. Integration: The retrieved data is combined with the user’s query and given to the \n",
      "RAG model to answer. Up to this point, the LLM has not processed the query.\n"
     ]
    }
   ],
   "source": [
    "query = \"How does the RAG model generate answers? Give stage 4 of the enumeration\"\n",
    "# Use invoke instead of deprecated method\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "# Print contents\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    print(f\"\\n--- Retrieved Chunk {i} ---\\n\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be002633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 4 ---\n",
      "\n",
      "4. Response: Blending the retrieved data with its own training and stored knowledge, \n",
      "the LLM generates a contextually accurate response. \n",
      "When searching through internal documents, RAG systems use semantic search. Vector \n",
      "databases organize data by similarity, thus enabling searches by meaning, rather than by \n",
      "keyword. Semantic search techniques enable RAG algorithms to reach past keywords to \n",
      "the intent of a query and return the most relevant data. \n",
      "RAG systems require extensive data architecture construction and maintenance. Data \n",
      "engineers must build the data pipelines needed to connect their organization’s data \n",
      "lakehouses with the LLM. \n",
      "To conceptualize RAG, imagine a gen AI model as an amateur home cook. They know the \n",
      "basics of cooking, but lack the expert knowledge—an organization’s proprietary \n",
      "database—of a chef trained in a particular cuisine. RAG is like giving the home cook a \n",
      "cookbook for that cuisine. By combining their general knowledge of cooking with the \n",
      "recipes in the cookbook, the home cook can create their favorite cuisine-speciﬁc dishes \n",
      "with ease. \n",
      "The RAG data retrieval process \n",
      "To use RAG eƯectively, data engineers must create data storage systems and pipelines that \n",
      "meet a series of important criteria. \n",
      "Enterprise data storage \n",
      "To enhance RAG system functions and enable real-time data retrieval, the data must be \n",
      "meticulously organized and maintained. Up-to-data metadata and minimal data \n",
      "redundancy help ensure eƯective querying. \n",
      "Document storage\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    if \"Response\" in doc.page_content:\n",
    "        print(f\"\\n--- Chunk {i} ---\\n\")\n",
    "        print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49f6da34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Retrieved Chunk 0 ---\n",
      "\n",
      "workﬂows and stay ahead of competitors, they often struggle with getting their chatbots \n",
      "and other models to reliably generate accurate answers. \n",
      "What’s the diƯerence between RAG and ﬁne-tuning? \n",
      "The diƯerence between RAG and ﬁne-tuning is that RAG augments a natural language\n",
      "\n",
      "--- Retrieved Chunk 1 ---\n",
      "\n",
      "workﬂows and stay ahead of competitors, they often struggle with getting their chatbots \n",
      "and other models to reliably generate accurate answers. \n",
      "What’s the diƯerence between RAG and ﬁne-tuning? \n",
      "The diƯerence between RAG and ﬁne-tuning is that RAG augments a natural language\n",
      "\n",
      "--- Retrieved Chunk 2 ---\n",
      "\n",
      "workﬂows and stay ahead of competitors, they often struggle with getting their chatbots \n",
      "and other models to reliably generate accurate answers. \n",
      "What’s the diƯerence between RAG and ﬁne-tuning? \n",
      "The diƯerence between RAG and ﬁne-tuning is that RAG augments a natural language\n",
      "\n",
      "--- Retrieved Chunk 3 ---\n",
      "\n",
      "to it. RAG models can return more accurate answers with the added context of internal \n",
      "data than they otherwise would be able to without it. \n",
      "A ﬁne-tuned model typically outperforms its corresponding base model, such as GPT-3 or\n",
      "\n",
      "--- Retrieved Chunk 4 ---\n",
      "\n",
      "to it. RAG models can return more accurate answers with the added context of internal \n",
      "data than they otherwise would be able to without it. \n",
      "A ﬁne-tuned model typically outperforms its corresponding base model, such as GPT-3 or\n",
      "\n",
      "--- Retrieved Chunk 5 ---\n",
      "\n",
      "have the same intended outcome: enhancing a model’s performance to maximize value for \n",
      "the enterprise that uses it. \n",
      "RAG uses an organization’s internal data to augment prompt engineering, while ﬁne-tuning \n",
      "retrains a model on a focused set of external data to improve performance. \n",
      " \n",
      "The latest AI News + Insights   \n",
      "Discover expertly curated insights and news on AI, cloud and more in the weekly Think \n",
      "Newsletter.  \n",
      "Subscribe today \n",
      "Why are RAG and ﬁne-tuning important? \n",
      "RAG plugs an LLM into stores of current, private data that would otherwise be inaccessible \n",
      "to it. RAG models can return more accurate answers with the added context of internal \n",
      "data than they otherwise would be able to without it. \n",
      "A ﬁne-tuned model typically outperforms its corresponding base model, such as GPT-3 or \n",
      "GPT-4, when applying its training with domain-speciﬁc data. The ﬁne-tuned LLM has a \n",
      "better understanding of the speciﬁc domain and its terminology, allowing it to generate \n",
      "accurate responses.\n",
      "\n",
      "--- Retrieved Chunk 6 ---\n",
      "\n",
      "RAG vs. ﬁne-tuning \n",
      "Retrieval augmented generation (RAG) and ﬁne-tuning are two methods enterprises can \n",
      "use to get more value out of large language models (LLMs). Both work by tailoring the LLM \n",
      "to the speciﬁc use cases, but the methodologies behind them diƯer signiﬁcantly.\n",
      "\n",
      "--- Retrieved Chunk 7 ---\n",
      "\n",
      "workﬂows and stay ahead of competitors, they often struggle with getting their chatbots \n",
      "and other models to reliably generate accurate answers. \n",
      "What’s the diƯerence between RAG and ﬁne-tuning? \n",
      "The diƯerence between RAG and ﬁne-tuning is that RAG augments a natural language \n",
      "processing (NLP) model by connecting it to an organization’s proprietary database, while \n",
      "ﬁne-tuning optimizes deep learning models for domain-speciﬁc tasks. RAG and ﬁne-tuning \n",
      "have the same intended outcome: enhancing a model’s performance to maximize value for \n",
      "the enterprise that uses it.\n",
      "\n",
      "--- Retrieved Chunk 8 ---\n",
      "\n",
      "the enterprise that uses it. \n",
      "RAG uses an organization’s internal data to augment prompt engineering, while ﬁne-tuning \n",
      "retrains a model on a focused set of external data to improve performance. \n",
      " \n",
      "The latest AI News + Insights\n",
      "\n",
      "--- Retrieved Chunk 9 ---\n",
      "\n",
      "Newsletter.  \n",
      "Subscribe today \n",
      "Why are RAG and ﬁne-tuning important? \n",
      "RAG plugs an LLM into stores of current, private data that would otherwise be inaccessible \n",
      "to it. RAG models can return more accurate answers with the added context of internal \n",
      "data than they otherwise would be able to without it. \n",
      "A ﬁne-tuned model typically outperforms its corresponding base model, such as GPT-3 or \n",
      "GPT-4, when applying its training with domain-speciﬁc data. The ﬁne-tuned LLM has a \n",
      "better understanding of the speciﬁc domain and its terminology, allowing it to generate \n",
      "accurate responses. \n",
      "Without continual access to new data, large language models stagnate. Modern LLMs are \n",
      "massive neural networks requiring huge data sets and computational resources to train.\n"
     ]
    }
   ],
   "source": [
    "query = \"Why are RAG and fine-tuning important?\"\n",
    "# Use invoke instead of deprecated method\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "# Print contents\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    print(f\"\\n--- Retrieved Chunk {i} ---\\n\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0039987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
