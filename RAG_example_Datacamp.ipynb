{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(api_key)  # Check if the key is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\python311.zip', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\DLLs', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\Lib', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0', 'c:\\\\Endava\\\\EndevLocal\\\\AI_speech_text_summarize\\\\venv', '', 'c:\\\\Endava\\\\EndevLocal\\\\AI_speech_text_summarize\\\\venv\\\\Lib\\\\site-packages', 'c:\\\\Endava\\\\EndevLocal\\\\AI_speech_text_summarize\\\\venv\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Endava\\\\EndevLocal\\\\AI_speech_text_summarize\\\\venv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Endava\\\\EndevLocal\\\\AI_speech_text_summarize\\\\venv\\\\Lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a powerful framework that simplifies the development of applications using large language models (LLMs). Here are three reasons to consider using LangChain for LLM application development:\n",
      "\n",
      "1. **Modular Components**: LangChain provides a modular approach to building LLM applications with its well-defined components. This allows developers to easily integrate and customize various functionalities like prompt management, memory handling, and chaining multiple LLM calls. This modularity promotes code reusability and simplifies the process of adding new features.\n",
      "\n",
      "2. **Tool and API Integration**: LangChain facilitates seamless integration with various tools, data sources, and APIs. Developers can connect LLMs with external databases, web services, or other platforms, enabling more complex use cases such as querying databases, retrieving live data, or interacting with other software. This capability enhances the versatility and functionality of LLM applications.\n",
      "\n",
      "3. **Chain and Agent Capabilities**: LangChain's ability to create chains and agents allows for sophisticated workflows designed to handle multi-step tasks. This feature enables applications to perform more complex operations, such as breaking down a question into several sub-questions or combining different models' capabilities. Such sophisticated flow control elevates the application's capability beyond simple question-answering to more nuanced and interactive experiences.\n",
      "\n",
      "These features make LangChain a valuable tool for developers looking to create robust and scalable LLM applications efficiently.\n"
     ]
    }
   ],
   "source": [
    "# Define the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Predict the words following the text in question\n",
    "prompt = 'Three reasons for using LangChain for LLM application development.'\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face is a great way to get a little bit of a break from the stresses of life. It's a great way to get a little bit of a break from the stresses of life.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the class for defining Hugging Face pipelines\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "# Define the LLM from the Hugging Face model ID\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"crumb/nano-mistral\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 40}\n",
    ")\n",
    "prompt = \"Hugging Face is\"\n",
    "# Invoke the model\n",
    "response = llm.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain simplifies Large Language Model (LLM) application development through several key features:\\n\\n1. **Modular Components**: LangChain provides modular components that developers can easily combine. These components can handle different tasks such as text generation, summarization, and question answering, allowing developers to build complex applications without crafting everything from scratch.\\n\\n2. **Chain Abstractions**: It introduces the concept of chains, which allow developers to define sequences of operations (e.g., input processing, LLM interaction, and output formatting). This enables the creation of more sophisticated pipelines where the output of one step can seamlessly feed into the next.\\n\\n3. **Integration with Various Tools**: LangChain supports integration with external data sources, APIs, and tools. This allows developers to enhance their LLM applications with additional data or capabilities, such as retrieving information from databases or incorporating search functionalities.\\n\\n4. **Memory Management**: LangChain includes mechanisms for managing state and memory within applications. This can be particularly useful for creating conversational agents that need to remember previous interactions or context between user queries.\\n\\n5. **Customization and Extensibility**: The platform is designed to be customizable, allowing developers to modify existing components or create new ones tailored to their specific needs, which leads to greater flexibility in application development.\\n\\n6. **Ease of Deployment**: LangChain also assists in deploying applications by providing guidelines and tools to streamline the process, making it easier for developers to move from prototyping to production.\\n\\n7. **Community and Resources**: Being an open-source framework, LangChain has a growing community and extensive documentation, tutorials, and examples which help developers quickly onboard and solve common challenges.\\n\\nOverall, LangChain accelerates the development process by abstracting complicating factors, enabling developers to focus more on their application logic and user experience rather than the intricacies of working directly with LLMs.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 375, 'prompt_tokens': 29, 'total_tokens': 404, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_92e0377081', 'finish_reason': 'stop', 'logprobs': None} id='run-b3623afb-5bf3-4889-8fa6-c42c7716fb93-0' usage_metadata={'input_tokens': 29, 'output_tokens': 375, 'total_tokens': 404, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Create a prompt template from the template string\n",
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template=template)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))\t\n",
    "# Create a chain to integrate the prompt template and LLM\n",
    "llm_chain = prompt | llm\n",
    "# Invoke the chain on the question\n",
    "question = \"How does LangChain make LLM application development easier?\"\n",
    "print(llm_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "North Korea has a limited automotive industry, primarily focused on domestic consumption due to international sanctions and isolation. Key facts about car manufacturing in North Korea include:\n",
      "\n",
      "1. **State-Owned Companies**: The country's primary car manufacturer is the Pyongyang Automotive Works, which produces various models under the brand name \"Pyeonghwa Motors.\" This company is a joint venture with South Korean interests but operates primarily with North Korean resources and labor.\n",
      "\n",
      "2. **Limited Model Variety**: The range of vehicles produced is quite limited, often consisting of older designs or modifications of foreign cars, such as the Soviet-era Moskvitch and the Fiat 124.\n",
      "\n",
      "3. **Focus on Utility Vehicles**: North Korean car manufacturing has historically focused more on utility and commercial vehicles than on passenger cars. The country produces vans, buses, and military vehicles, which are more suited to the needs of the state.\n",
      "\n",
      "4. **Self-Reliance Ideology**: North Korea emphasizes self-reliance (Juche ideology), which reflects in its efforts to develop its automotive sector, despite facing significant technological and economic challenges.\n",
      "\n",
      "5. **Infrastructure Weakness**: The automotive sector in North Korea is significantly hampered by poor infrastructure and limited access to modern technology or investment, which means that production levels and vehicle quality are much lower than in more developed automotive nations.\n"
     ]
    }
   ],
   "source": [
    "# Create a chat prompt template\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a country expert that return key car manufature facts about a country.\"),\n",
    "        (\"human\", \"France\"),\n",
    "        (\"ai\", \"Peugeot, Reanult, Citroen are the leading car brands produced in France\"),\n",
    "        (\"human\", \"{country}\")\n",
    "    ]\n",
    ")\n",
    "# Chain the prompt template and model, and invoke the chain\n",
    "llm_chain = prompt_template | llm\n",
    "country = \"Noth Korea\"\n",
    "response = llm_chain.invoke({\"country\": country})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the examples list of dicts\n",
    "examples = [\n",
    "  {\n",
    "    \"question\": \"How many DataCamp courses has Jack completed?\",\n",
    "    \"answer\": \"36\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"How much XP does Jack have on DataCamp?\",\n",
    "    \"answer\": \"284,320XP\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What technology does Jack learn about most on DataCamp?\",\n",
    "    \"answer\": \"Python\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many DataCamp courses has Jack completed?\n",
      "36\n",
      "\n",
      "Question: How much XP does Jack have on DataCamp?\n",
      "284,320XP\n",
      "\n",
      "Question: What technology does Jack learn about most on DataCamp?\n",
      "Python\n",
      "\n",
      "Question: What is Jack's favorite technology on DataCamp?\n"
     ]
    }
   ],
   "source": [
    "# Complete the prompt for formatting answers\n",
    "example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")\n",
    "\n",
    "# Create the few-shot prompt\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"input\": \"What is Jack's favorite technology on DataCamp?\"})\n",
    "print(prompt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Jack's favorite technology on DataCamp is Python.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 64, 'total_tokens': 75, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None} id='run-0c39e958-f902-44e6-8b92-2ba9dd4c0156-0' usage_metadata={'input_tokens': 64, 'output_tokens': 11, 'total_tokens': 75, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "llm_chain = prompt_template | llm\n",
    "print(llm_chain.invoke({\"input\": \"What is Jack's favorite technology on DataCamp?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='I want to learn how to play golf. Can you suggest how I can learn this step-by-step?'\n"
     ]
    }
   ],
   "source": [
    "# Create a prompt template that takes an input activity\n",
    "learning_prompt = PromptTemplate(\n",
    "    input_variables=[\"activity\"],\n",
    "    template=\"I want to learn how to {activity}. Can you suggest how I can learn this step-by-step?\"\n",
    ")\n",
    "\n",
    "# Create a prompt template that places a time constraint on the output\n",
    "time_prompt = PromptTemplate(\n",
    "    input_variables=[\"learning_plan\"],\n",
    "    template=\"I only have one week. Can you create a plan to help me hit this goal: {learning_plan}.\"\n",
    ")\n",
    "\n",
    "# Invoke the learning_prompt with an activity\n",
    "print(learning_prompt.invoke({\"activity\": \"play golf\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a concise 7-day plan to help you learn how to engineer prompts for large language models (LLMs):\n",
      "\n",
      "### Day 1: Understand the Basics of LLMs\n",
      "- **Read Introductory Material**: Spend 2-3 hours reading about LLMs, their architectures, training mechanisms, and applications. Key resources:\n",
      "  - \"Attention is All You Need\" paper by Vaswani et al.\n",
      "  - Introductory online courses in NLP (e.g., Coursera or edX).\n",
      "- **Watch Introductory Videos**: Look for videos on YouTube explaining LLMs and transformers.\n",
      "\n",
      "### Day 2: Explore Existing Prompts\n",
      "- **Experiment with LLMs**: Spend 2-3 hours using platforms like OpenAI’s ChatGPT or Hugging Face’s models. Try different prompts and note variations in responses.\n",
      "- **Research Prompt Examples**: Explore community forums like Reddit or GitHub to see examples and discussions about effective prompts.\n",
      "\n",
      "### Day 3: Learn the Principles of Prompt Engineering\n",
      "- **Study Prompt Structures**: Devote around 2 hours to learning effective prompt structures (context setting, explicit instructions, etc.).\n",
      "- **Experiment**: Spend an hour creating prompts of varying lengths and tones. Take notes on the outcomes.\n",
      "\n",
      "### Day 4: Hands-On Practice\n",
      "- **Create a Prompt Library**: Allocate 3-4 hours to build a library of prompts tailored to different tasks (summarization, Q&A, etc.).\n",
      "- **Testing**: Use LLMs to test your library of prompts and refine them based on the output.\n",
      "\n",
      "### Day 5: Learn from the Community and Advanced Techniques\n",
      "- **Join Online Forums**: Spend 1-2 hours engaging with communities on Reddit, Discord, or Twitter. Share your findings and ask questions.\n",
      "- **Study Advanced Prompting Techniques**: Allocate another 2 hours researching few-shot, zero-shot, and chain of thought prompting techniques.\n",
      "\n",
      "### Day 6: Analyze and Evaluate\n",
      "- **Learn Evaluation Metrics**: Spend 1-2 hours understanding qualitative and quantitative metrics to assess prompt quality.\n",
      "- **Evaluate Your Outputs**: Review your previous prompt outputs based on these metrics and identify areas of improvement.\n",
      "\n",
      "### Day 7: Build a Portfolio and Apply Your Skills\n",
      "- **Create a Documentation Portfolio**: Spend 2-3 hours compiling your best prompts, projects, and a write-up of your learning journey.\n",
      "- **Consider Real-World Applications**: Think of projects you could develop using your skills—chatbots, content generators, etc. Write down ideas for collaboration if you're interested in working with others.\n",
      "\n",
      "By following this intensive plan, you'll have a solid foundation in prompt engineering for LLMs at the end of the week. Good luck!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "learning_prompt = PromptTemplate(\n",
    "    input_variables=[\"activity\"],\n",
    "    template=\"I want to learn how to {activity}. Can you suggest how I can learn this step-by-step?\"\n",
    ")\n",
    "\n",
    "time_prompt = PromptTemplate(\n",
    "    input_variables=[\"learning_plan\"],\n",
    "    template=\"I only have one week. Can you create a concise plan to help me hit this goal: {learning_plan}.\"\n",
    ")\n",
    "\n",
    "# Complete the sequential chain with LCEL\n",
    "seq_chain = ({\"learning_plan\": learning_prompt | llm | StrOutputParser()}\n",
    "    | time_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "# Call the chain\n",
    "print(seq_chain.invoke({\"activity\": \"engineer llm prompting\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_tools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define the tools\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tools \u001b[38;5;241m=\u001b[39m \u001b[43mload_tools\u001b[49m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwikipedia\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define the agent\u001b[39;00m\n\u001b[0;32m      5\u001b[0m prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow many people live in New York City?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_tools' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the tools\n",
    "tools = load_tools([\"wikipedia\"])\n",
    "\n",
    "# Define the agent\n",
    "prompt=\"How many people live in New York City?\"\n",
    "agent = create_react_agent(llm, tools,prompt=prompt)\n",
    "\n",
    "# Invoke the agent\n",
    "\n",
    "response = agent.invoke({\"messages\": [(\"human\", \"How many people live in New York City?\")]})\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   name  location  revenue\n",
      "0  Peak Performance Co.  New York  1000000\n",
      "3  Peak Performance Co.     Miami  1200000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame to mimic customer data\n",
    "data = {\n",
    "    'name': ['Peak Performance Co.', 'Fitness Corp', 'Wellness Ltd.', 'Peak Performance Co.'],\n",
    "    'location': ['New York', 'Los Angeles', 'Chicago', 'Miami'],\n",
    "    'revenue': [1000000, 500000, 750000, 1200000],\n",
    "}\n",
    "\n",
    "customers = pd.DataFrame(data)\n",
    "\n",
    "# Define a function to retrieve customer info by-name\n",
    "def retrieve_customer_info(name: str) -> str:\n",
    "    \"\"\"Retrieve customer information based on their name.\"\"\"\n",
    "    # Filter customers for the customer's name\n",
    "    customer_info = customers[customers['name'] == name]\n",
    "    return customer_info.to_string()\n",
    "  \n",
    "# Call the function on Peak Performance Co.\n",
    "print(retrieve_customer_info(\"Peak Performance Co.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   name  location  revenue\n",
      "0  Peak Performance Co.  New York  1000000\n",
      "3  Peak Performance Co.     Miami  1200000\n",
      "(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Convert the retrieve_customer_info function into a tool\n",
    "def tool(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "@tool\n",
    "def retrieve_customer_info(name: str) -> str:\n",
    "    \"\"\"Retrieve customer information based on their name.\"\"\"\n",
    "    customer_info = customers[customers['name'] == name]\n",
    "    return customer_info.to_string()\n",
    "  \n",
    "# Print the tool's arguments\n",
    "print(retrieve_customer_info(\"Peak Performance Co.\"))\n",
    "import inspect\n",
    "print(inspect.signature(retrieve_customer_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;129m@tool\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mretrieve_customer_info\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve customer information based on their name.\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     customer_info \u001b[38;5;241m=\u001b[39m customers[customers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m name]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tool' is not defined"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def retrieve_customer_info(name: str) -> str:\n",
    "    \"\"\"Retrieve customer information based on their name.\"\"\"\n",
    "    customer_info = customers[customers['name'] == name]\n",
    "    return customer_info.to_string()\n",
    "\n",
    "# Create a ReAct agent\n",
    "agent = create_react_agent(llm, [retrieve_customer_info])\n",
    "\n",
    "# Invoke the agent on the input\n",
    "messages = agent.invoke({\"messages\": [(\"human\", \"Create a summary of our customer: Peak Performance Co.\")]})\n",
    "print(messages['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='' metadata={'producer': 'Microsoft: Print To PDF', 'creator': 'PyPDF', 'creationdate': '2025-03-17T09:20:16+02:00', 'author': 'Orest Cramar', 'moddate': '2025-03-17T09:20:16+02:00', 'title': 'chapter2.pdf', 'source': 'rag_vs_fine_tuning.pdf', 'total_pages': 53, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Create a document loader for rag_vs_fine_tuning.pdf\n",
    "loader = PyPDFLoader(\"rag_vs_fine_tuning.pdf\")\n",
    "\n",
    "# Load the document\n",
    "data = loader.load()\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 57, which is longer than the specified 24\n",
      "Created a chunk of size 29, which is longer than the specified 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words are flowing out like endless rain into a paper cup,\n",
      "[57, 29, 35]\n"
     ]
    }
   ],
   "source": [
    "# Import the character splitter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "quote = 'Words are flowing out like endless rain into a paper cup,\\nthey slither while they pass,\\nthey slip away across the universe.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 10\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split the string and print the chunks\n",
    "docs = splitter.split_text(quote)\n",
    "print(docs[0])\n",
    "print([len(doc) for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Words are flowing out', 'out like endless rain', 'rain into a paper cup,', 'they slither while they', 'they pass,', 'they slip away across', 'across the universe.']\n",
      "[21, 21, 22, 23, 10, 21, 20]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "quote = 'Words are flowing out like endless rain into a paper cup,\\nthey slither while they pass,\\nthey slip away across the universe.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 10\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\",\"\\n\",\" \",\"\",],\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split the document and print the chunks\n",
    "docs = splitter.split_text(quote)\n",
    "print(docs)\n",
    "print([len(doc) for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ocramar\\AppData\\Local\\Temp\\ipykernel_13912\\763544368.py:18: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7ebad1e230414b8c2e1cfc2a9efec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Endava\\EndevLocal\\AI_speech_text_summarize\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ocramar\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067f73367e2846cdabe7251364534e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eed6d971a3b4a9b8f1dff91df593c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a435a704dd4e76b94b90591165465b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efaeaa286b24243b4675e76501f828b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46c9ea4f5d34ba095b96a0659d0ce0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbf85e5f70c493aae8a11e36deb529c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec01a2288c2842bcb89c6c00b905cd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a63a9d78e7042b28474110c6b08f56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1220e184703041af8f5d69debdbdb819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7013914d87145819acae9091e5a9760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ccc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 40 documents...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embedding {len(docs)} documents...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document 1 ---\n",
      "\n",
      "RAG vs. ﬁne-tuning \n",
      "Retrieval augmented generation (RAG) and ﬁne-tuning are two methods enterprises can \n",
      "use to get more value out of large language models (LLMs). Both work by tailoring the LLM \n",
      "to the speciﬁc use cases, but the methodologies behind them diƯer signiﬁcantly.\n",
      "\n",
      "--- Document 2 ---\n",
      "\n",
      "Watch the latest podcast episodes  \n",
      "What is retrieval augmented generation (RAG)? \n",
      "RAG is an LLM optimization method introduced by Meta AI in a 2020 paper called \n",
      "\"Retrieval-Augmented Generation for Knowledge-Intensive Tasks\" .[1] It is a data architecture\n",
      "\n",
      "--- Document 3 ---\n",
      "\n",
      "What is ﬁne-tuning? \n",
      "Fine-tuning is the process of retraining a pretrained model on a smaller, more focused set \n",
      "of training data to give it domain-speciﬁc knowledge. The model then adjusts its \n",
      "parameters—the guidelines governing its behavior—and its embeddings to better ﬁt the \n",
      "speciﬁc data set.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Example question\n",
    "query = \"What is the difference between retrieval-augmented generation and fine-tuning?\"\n",
    "\n",
    "# Updated way to retrieve relevant documents\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "# Print out the contents of the most relevant documents\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    print(f\"\\n--- Document {i+1} ---\\n\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
